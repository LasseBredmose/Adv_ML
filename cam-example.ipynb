{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98124d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.data.dataloader import MURADataset\n",
    "from src.models.models import CNN\n",
    "from netcal.metrics import ECE\n",
    "from src.models.utils import pred, get_variable\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import cv2\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6e4e027",
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.989 -> 0\n",
      "0.011 -> 4\n",
      "0.000 -> 1\n",
      "0.000 -> 3\n",
      "0.000 -> 2\n",
      "()\n",
      "output CAM.jpg for the top1 prediction: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple implementation of CAM in PyTorch for the networks such as ResNet, DenseNet, SqueezeNet, Inception\n",
    "# last update by BZ, June 30, 2021\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "# input image\n",
    "#LABELS_file = 'imagenet-simple-labels.json'\n",
    "image_file = 'data/MURA-v1.1/valid/XR_SHOULDER/patient11723/study1_positive/image3.png'\n",
    "\n",
    "model_path = 'models/STATEtrained_model_epocs2_24-03-2022_14.pt'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "net = CNN(input_channels=3, input_height=256, input_width=256, num_classes=7).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "net.eval()\n",
    "\n",
    "net.load_state_dict(\n",
    "    torch.load(\n",
    "        model_path,\n",
    "        map_location=torch.device(device),\n",
    "    )\n",
    ")\n",
    "\n",
    "net.eval()\n",
    "\n",
    "# hook the feature extractor\n",
    "features_blobs = []\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "net._modules.get('conv3').register_forward_hook(hook_feature)\n",
    "\n",
    "# get the softmax weight\n",
    "params = list(net.parameters())\n",
    "weight_softmax = np.squeeze(params[-2].data.numpy())\n",
    "\n",
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        print(weight_softmax[idx].shape)\n",
    "        cam = weight_softmax.dot(feature_conv.reshape((nc, h*w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam\n",
    "\n",
    "\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# load test image\n",
    "img_pil = Image.open(image_file).convert(\"RGB\")\n",
    "img_tensor = preprocess(img_pil)\n",
    "img_variable = Variable(img_tensor.unsqueeze(0))\n",
    "logit = net(img_variable)\n",
    "\n",
    "# load the imagenet category list\n",
    "classes = [0,1,2,3,4,5,6]\n",
    "\n",
    "\n",
    "h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "probs, idx = h_x.sort(0, True)\n",
    "probs = probs.numpy()\n",
    "idx = idx.numpy()\n",
    "\n",
    "# output the prediction\n",
    "for i in range(0, 5):\n",
    "    print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))\n",
    "\n",
    "# generate class activation mapping for the top1 prediction\n",
    "CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0]])\n",
    "\n",
    "# render the CAM and output\n",
    "print('output CAM.jpg for the top1 prediction: %s'%classes[idx[0]])\n",
    "img = cv2.imread(image_file)\n",
    "height, width, _ = img.shape\n",
    "heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
    "result = heatmap * 0.3 + img * 0.5\n",
    "cv2.imwrite('reports/CAM.jpg', result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6935b997",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f0b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    print(weight_softmax.shape)\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        cam = weight_softmax.dot(feature_conv.reshape((nc, h*w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be2909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_path = 'models/STATEtrained_model_epocs2_24-03-2022_14.pt'\n",
    "\n",
    "# Inputs\n",
    "batch_size = 1\n",
    "shuffle = True\n",
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75faa003",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Transforming the data, such that they all follow the same path\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_set = MURADataset(\n",
    "    \"data\",\n",
    "    \"MURA-v1.1/valid_labeled_studies.csv\",\n",
    "    \"MURA-v1.1/valid_image_paths.csv\",\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_set,\n",
    "    shuffle=False,  # very important!\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "model = CNN(input_channels=3, input_height=256, input_width=256, num_classes=7).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        model_path,\n",
    "        map_location=torch.device(device),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ab954",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(model.parameters())\n",
    "weight_softmax = np.squeeze(params[-2].data.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_blobs = []\n",
    "\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "model._modules.get('conv3').register_forward_hook(hook_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8231d14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(test_loader))\n",
    "\n",
    "inputs, labels = Variable(get_variable(inputs)), Variable(\n",
    "                get_variable(labels)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f6a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = model(inputs)\n",
    "h_x = F.softmax(logit, dim=1)\n",
    "probs, idx = h_x.sort(0, True)\n",
    "\n",
    "# generate class activation mapping for the top1 prediction\n",
    "CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c3a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# render the CAM and output\n",
    "print('output CAM.jpg for the top1 prediction: %s'%labels[idx[0]])\n",
    "img = cv2.imread('data/MURA-v1.1/valid/XR_WRIST/patient11185/study1_positive/image1.png')\n",
    "height, width, _ = img.shape\n",
    "heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
    "result = heatmap * 0.3 + img * 0.5\n",
    "cv2.imwrite('CAM.jpg', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ce9c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAMs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fd0b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_softmax[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b391681",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8070f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader.item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310e7f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb39ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c854c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3a85d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fdd07d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c17e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
