{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "758bfa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.data.dataloader import MURADataset\n",
    "from src.models.models import CNN, CNN_3\n",
    "from netcal.metrics import ECE\n",
    "from src.models.utils import pred, get_variable\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64560c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bn5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout2d(p=0.2, inplace=False)\n",
       "  (avgpool): AvgPool2d(kernel_size=32, stride=32, padding=0)\n",
       "  (l_out): Linear(in_features=64, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input image\n",
    "#image_file = 'data/MURA-v1.1/valid/XR_ELBOW/patient11204/study1_negative/image1.png'\n",
    "#image_file = 'data/MURA-v1.1/train/XR_FINGER/patient04887/study1_negative/image2.png'\n",
    "#image_file = 'data/MURA-v1.1/valid/XR_HAND/patient11212/study1_negative/image2.png'\n",
    "image_file = 'data/MURA-v1.1/valid/XR_SHOULDER/patient11723/study1_positive/image3.png'\n",
    "#image_file = 'data/MURA-v1.1/valid/XR_WRIST/patient11185/study1_positive/image1.png'\n",
    "\n",
    "#model_path = 'models/STATEtrained_model_epocs2_24-03-2022_14.pt'\n",
    "#model_path = 'models/STATEtrained_model_epocs70_24-03-2022_22.pt'\n",
    "\n",
    "# The model that we want to use\n",
    "#model_path = 'models/STATEtrained_model_epocs100_16_04_23_trans_0_layers_3.pt' # 3 layer no trans\n",
    "#model_path = 'models/STATEtrained_model_epocs100_16_04_22_trans_1_layers_3.pt' # 3 layer trans\n",
    "#model_path = 'models/STATEtrained_model_epocs100_16_04_22_trans_0_layers_5.pt' # 5 layer no trans\n",
    "#model_path = 'models/STATEtrained_model_epocs100_16_04_22_trans_1_layers_5.pt' # 5 layer trans\n",
    "\n",
    "# BNN\n",
    "model_path = f'models/tester_{method}.pt'\n",
    "\n",
    "# Model on only two epochs:\n",
    "#model_path = 'models/STATEtrained_model_epocs2_15_04_15_trans_0.pt'\n",
    "#model_path = 'models/STATEtrained_model_epocs2_15_04_20_trans_0.pt'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "net = CNN(input_channels=3, input_height=256, input_width=256, num_classes=7).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "last_conv_name = 'conv5'\n",
    "\n",
    "net.load_state_dict(\n",
    "    torch.load(\n",
    "        model_path,\n",
    "        map_location=torch.device(device),\n",
    "    )\n",
    ")\n",
    "\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c73b8d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_blobs = []\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "net._modules.get(last_conv_name).register_forward_hook(hook_feature)\n",
    "\n",
    "# get the softmax weight\n",
    "params = list(net.parameters())\n",
    "weight_softmax = np.squeeze(params[-2].data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04645785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.971 -> 0 (SHOULDER)\n",
      "0.021 -> 5 (FOREARM)\n",
      "0.004 -> 1 (HUMERUS)\n",
      "0.002 -> 6 (HAND)\n",
      "0.001 -> 2 (FINGER)\n",
      "output CAM.jpg for the top1 prediction: SHOULDER\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h*w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam\n",
    "\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# load test image\n",
    "img_pil = Image.open(image_file).convert(\"RGB\")\n",
    "img_tensor = preprocess(img_pil)\n",
    "img_variable = Variable(img_tensor.unsqueeze(0))\n",
    "logit = net(img_variable)\n",
    "\n",
    "# load list of classes\n",
    "classes = [0,1,2,3,4,5,6]\n",
    "\n",
    "\n",
    "h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "probs, idx = h_x.sort(0, True)\n",
    "probs = probs.numpy()\n",
    "idx = idx.numpy()\n",
    "\n",
    "mapper = {0: 'SHOULDER',\n",
    "         1: 'HUMERUS',\n",
    "         2: 'FINGER',\n",
    "         3: 'ELBOW',\n",
    "         4: 'WRIST',\n",
    "         5: 'FOREARM',\n",
    "         6: 'HAND'}\n",
    "\n",
    "# output the prediction\n",
    "for i in range(0, 5):\n",
    "    print('{:.3f} -> {} ({})'.format(probs[i], classes[idx[i]], mapper[classes[idx[i]]]))\n",
    "\n",
    "# generate class activation mapping for the top1 prediction\n",
    "CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0]])\n",
    "\n",
    "# render the CAM and output\n",
    "print('output CAM.jpg for the top1 prediction: %s'%mapper[classes[idx[0]]])\n",
    "img = cv2.imread(image_file)\n",
    "height, width, _ = img.shape\n",
    "heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
    "result = heatmap * 0.3 + img * 0.5\n",
    "cv2.imwrite(f'CAM_{method}_2.jpg', result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdb86e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'XR_SHOULDER_patient11723_study1_positive_image3.png'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'_'.join(image_file.split('/')[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99b8a4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'MURA-v1.1',\n",
       " 'valid',\n",
       " 'XR_SHOULDER',\n",
       " 'patient11723',\n",
       " 'study1_positive',\n",
       " 'image3.png']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_file.split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d7e9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
