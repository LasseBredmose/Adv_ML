{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98124d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.data.dataloader import MURADataset\n",
    "from src.models.models import CNN, CNN_3\n",
    "from netcal.metrics import ECE\n",
    "from src.models.utils import pred, get_variable\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6e4e027",
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 32, 32])\n",
      "torch.Size([1, 7])\n",
      "0.170 -> 4\n",
      "0.169 -> 0\n",
      "0.152 -> 5\n",
      "0.132 -> 1\n",
      "0.132 -> 3\n",
      "output CAM.jpg for the top1 prediction: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple implementation of CAM in PyTorch for the networks such as ResNet, DenseNet, SqueezeNet, Inception\n",
    "# last update by BZ, June 30, 2021\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "# input image\n",
    "#image_file = 'data/MURA-v1.1/valid/XR_ELBOW/patient11204/study1_negative/image1.png'\n",
    "#image_file = 'data/MURA-v1.1/train/XR_FINGER/patient04887/study1_negative/image2.png'\n",
    "#image_file = 'data/MURA-v1.1/valid/XR_HAND/patient11212/study1_negative/image2.png'\n",
    "image_file = 'data/MURA-v1.1/valid/XR_SHOULDER/patient11723/study1_positive/image3.png'\n",
    "\n",
    "#model_path = 'models/STATEtrained_model_epocs2_24-03-2022_14.pt'\n",
    "#model_path = 'models/STATEtrained_model_epocs70_24-03-2022_22.pt'\n",
    "\n",
    "# The model that we want to use\n",
    "#model_path = 'models/STATEtrained_model_epocs100_14_04_18_trans_1.pt'\n",
    "\n",
    "# Model on only two epochs:\n",
    "model_path = 'models/STATEtrained_model_epocs2_15_04_13_trans_0.pt'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "net = CNN(input_channels=3, input_height=256, input_width=256, num_classes=7).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "net.eval()\n",
    "\n",
    "net.load_state_dict(\n",
    "    torch.load(\n",
    "        model_path,\n",
    "        map_location=torch.device(device),\n",
    "    )\n",
    ")\n",
    "\n",
    "net.eval()\n",
    "\n",
    "# hook the feature extractor\n",
    "features_blobs = []\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "net._modules.get('conv5').register_forward_hook(hook_feature)\n",
    "\n",
    "# get the softmax weight\n",
    "params = list(net.parameters())\n",
    "weight_softmax = np.squeeze(params[-2].data.numpy())\n",
    "\n",
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        cam = weight_softmax.dot(feature_conv.reshape((nc, h*w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam\n",
    "\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# load test image\n",
    "img_pil = Image.open(image_file).convert(\"RGB\")\n",
    "img_tensor = preprocess(img_pil)\n",
    "img_variable = Variable(img_tensor.unsqueeze(0))\n",
    "logit = net(img_variable)\n",
    "\n",
    "# load the imagenet category list\n",
    "classes = [0,1,2,3,4,5,6]\n",
    "\n",
    "\n",
    "h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "probs, idx = h_x.sort(0, True)\n",
    "probs = probs.numpy()\n",
    "idx = idx.numpy()\n",
    "\n",
    "# output the prediction\n",
    "for i in range(0, 5):\n",
    "    print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))\n",
    "\n",
    "# generate class activation mapping for the top1 prediction\n",
    "CAMs = returnCAM(np.abs(features_blobs[0]), weight_softmax, [idx[0]])\n",
    "\n",
    "# render the CAM and output\n",
    "print('output CAM.jpg for the top1 prediction: %s'%classes[idx[0]])\n",
    "img = cv2.imread(image_file)\n",
    "height, width, _ = img.shape\n",
    "heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
    "result = heatmap * 0.3 + img * 0.5\n",
    "cv2.imwrite('reports/CAM.jpg', result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "843e3cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net._modules.get(finalconv_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3259f08e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 32, 32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_blobs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3f367ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "  (conv4): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv5): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (dropout): Dropout2d(p=0.2, inplace=False)\n",
       "  (l_out): Linear(in_features=16384, out_features=7, bias=False)\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "331288eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "  (conv4): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv5): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (dropout): Dropout2d(p=0.2, inplace=False)\n",
       "  (l_out): Linear(in_features=16384, out_features=7, bias=False)\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dc05b910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, img_size, num_class):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # 3 x ? x ?\n",
    "            nn.Conv2d(3, 32, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # 32 x ? x ?\n",
    "            nn.Conv2d(32, 64, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # 64 x ? x ?\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # 64 x ? / 2 x ? / 2\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # 128 x ? / 2 x ? / 2\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # 256 x ? / 2 x ? / 2\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # 256 x ? / 4 x ? / 4\n",
    "            nn.Conv2d(256, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # 512 x ? / 4 x ? / 4\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # 512 x ? / 8 x ? / 8\n",
    "            nn.Conv2d(512, num_class, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.avg_pool = nn.AvgPool2d(img_size // 8)\n",
    "        self.classifier = nn.Linear(num_class, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.conv(x)\n",
    "        flatten = self.avg_pool(features).view(features.size(0), -1)\n",
    "        output = self.classifier(flatten)\n",
    "        return output, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0aa531d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CNN(128, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f586fcc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): LeakyReLU(negative_slope=0.2)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): LeakyReLU(negative_slope=0.2)\n",
       "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): LeakyReLU(negative_slope=0.2)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): LeakyReLU(negative_slope=0.2)\n",
       "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (13): Conv2d(512, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (avg_pool): AvgPool2d(kernel_size=16, stride=16, padding=0)\n",
       "  (classifier): Linear(in_features=100, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31c99f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
